"""Trial execution and result management for SWE-bench evaluation.

This module provides classes for executing and evaluating individual test trials:
- TrialResult: Represents the outcome of a trial execution
- Trial: Manages the execution of a single test case

The module handles:
- Docker container lifecycle
- Test environment setup
- Patch application and validation
- Test execution and result collection
- Result evaluation and grading

Typical usage:
    trial = Trial(instance, "test-1", "./results")
    result = trial.run()
    if result.success:
        print("Test passed!")
"""

import logging
import os
import json
import subprocess
from dataclasses import dataclass, asdict
from typing import Any

from .swe_bench_instance import SWEBenchInstance
from .docker_instance import DockerInstance

from swebench.harness.grading import get_eval_report
from swebench.harness.test_spec.test_spec import (
    make_test_spec,
    TestSpec,
)

@dataclass
class TrialResult:
    """Represents the outcome of a trial execution.
    
    This class encapsulates all information about a trial's execution,
    including success/failure status, error messages, and the generated patch.
    
    Attributes:
        instance: The SWE-bench instance that was tested
        run_failed: Whether there was an error during execution
        validation_failed: Whether the test validation failed
        success: Whether the trial successfully passed all tests
        error: Error message if any failure occurred
        patch: The patch generated by the agent
    """
    instance: SWEBenchInstance
    run_failed: bool = False
    validation_failed: bool = False
    success: bool = False
    error: str | None = None
    patch: str | None = None

    def failed(self) -> bool:
      """Check if the trial failed in any way.
      
      Returns:
          bool: True if there was any kind of failure (run, validation, or error),
                False if the trial completed successfully
      """
      return self.run_failed or self.validation_failed or (self.error is not None)

    def to_dict(self) -> dict[str, Any]:
      """Convert the TrialResult to a dictionary for JSON serialization.
      
      This method handles nested objects by checking for and using their
      to_dict methods when available.
      
      Returns:
          dict[str, Any]: A JSON-serializable dictionary containing all trial
                         result data, including nested objects
      """
      result = asdict(self)
      # Convert the SWEBenchInstance to a dict if it has a to_dict method
      if hasattr(self.instance, 'to_dict'):
          result['instance'] = self.instance.to_dict()
      return result

class Trial:
  """Manages the execution of a single SWE-bench test case.
  
  This class handles the complete lifecycle of a test trial, including:
  - Docker container setup and cleanup
  - Test environment preparation
  - Patch application and validation
  - Test execution and result collection
  - Result evaluation and grading
  
  The class integrates with the SWE-bench evaluation framework to ensure
  consistent test execution and result reporting.
  """
  item: SWEBenchInstance
  name: str
  container: DockerInstance
  results_dir: str

  def __init__(self, item: SWEBenchInstance, name: str, results_dir: str) -> None:
    """Initialize a new trial.
    
    Args:
        item: The SWE-bench instance to test
        name: Unique identifier for this trial
        results_dir: Directory where results and artifacts will be stored
    """
    self.item = item
    self.name = name
    self.results_dir = results_dir
    self.container = DockerInstance(self.item, self.results_dir)

  def run(self) -> TrialResult:
    """Execute the trial.
    
    This method performs the complete trial execution:
    1. Sets up the Docker container
    2. Applies the test patch
    3. Establishes initial git state
    4. Runs pre-patch tests
    5. Installs and runs the agent
    6. Collects and evaluates results
    
    Returns:
        TrialResult: The complete results of the trial execution
    """
    logging.info(f"Running trial {self.name}")

    try:
        self.container.run(self.name)

        # First establish initial git state
        initial_git_ref = self.establish_initial_git_ref()

        # Then apply the patch
        self.container.write_string_to_file(self.item.test_patch, "/tmp/test.patch")
        # Try to apply the patch and get detailed error if it fails
        patch_result = self.container.exec("git apply /tmp/test.patch")
        if patch_result.exit_code != 0:
          # Get more details about the failure
          logging.info(f"Test Patch failed with code {patch_result.exit_code}")
          logging.info(f"Patch output: {patch_result.output}")
          
          # Try with -v for more details
          verbose_result = self.container.exec("git apply -v /tmp/test.patch")
          logging.info(f"Verbose patch output: {verbose_result.output}")
          
          return TrialResult(
            instance=self.item,
            run_failed=False,
            validation_failed=True,
            error=f"Patch failed: {patch_result.output}",
          )
        
        pre_patch_results = self.container.exec(self.item.test_cmd)
        pre_patch_results_path = os.path.join(self.results_dir, f"{self.name}-pre_patch_test_results.txt")
        
        # write results to file in results_dir
        with open(pre_patch_results_path, "w") as f:
          f.write(pre_patch_results.output.decode())

        # Run the agent
        self.install_agent()
        self.run_agent()

        # Get the changes made by the agent
        diff = self.container.exec(f"git diff {initial_git_ref} HEAD").output.decode()

        prediction = {
          "instance_id": self.item.instance_id,
          "model_name_or_path": self.name,
          "model_patch": diff,
        }

        test_results = self.container.exec(self.item.test_cmd).output.decode()
        test_results_path = os.path.join(self.results_dir, f"{self.name}-test_results.txt")
        
        with open(test_results_path, "w") as f:
          f.write(test_results)

        model_patch_path = os.path.join(self.results_dir, f"{self.name}-patch.diff")

        with open(model_patch_path, "w") as f:
          f.write(diff)

        result = self.evaluate_results(prediction, test_results_path)
        
        # TODO: Uncomment next line when debugging is done:
        # self.container.cleanup()

        return result
    except Exception as e:
        return TrialResult(
            instance=self.item,
            run_failed=True,
            validation_failed=False,
            error=str(e)
        )

  def establish_initial_git_ref(self) -> str:
    """Create a git commit of the current state and get its reference.
    
    This method:
    1. Configures git user information
    2. Creates a commit with the current state
    3. Returns the commit hash
    
    Returns:
        str: The hash of the created commit
        
    Raises:
        Exception: If git operations fail
    """
    # Configure git user
    result = self.container.exec("git config user.name 'agent-test-harness'")
    if result.exit_code != 0:
        raise Exception(f"Failed to configure git user name: {result.output}")

    result = self.container.exec("git config user.email 'agent-test-harness@bosun.ai'")
    if result.exit_code != 0:
        raise Exception(f"Failed to configure git user email: {result.output}")

    # Create initial commit - git commit may return non-zero even on success
    self.container.exec("git commit -a -m 'benchmark-head'")

    # Get commit hash - this will fail if there really was no commit
    result = self.container.exec("git rev-parse HEAD")
    if result.exit_code != 0:
        raise Exception(f"Failed to get commit hash: {result.output}")

    return result.output.decode().strip()

  def install_agent(self) -> None:
    """Install the Kwaak agent in the test environment.
    
    This method:
    1. Locates the agent source in the git repository
    2. Ensures cross-compilation tools are available
    3. Builds the agent for x86_64 Linux
    4. Copies the binary to the container directory
    
    Raises:
        subprocess.CalledProcessError: If any build step fails
    """
    # agent is located at the root of the git repository of the cwd
    agent_root = subprocess.check_output(["git", "rev-parse", "--show-toplevel"]).decode().strip()

    # check if cross is installed
    if subprocess.run(["cross", "--version"], check=False).returncode != 0:
      subprocess.run(["cargo", "install", "cross", "--git", "https://github.com/cross-rs/cross"], check=True)

    # copy the agent binary to the root of the results directory
    agent_path = os.path.join(agent_root, "target", "x86_64-unknown-linux-gnu", "release", "kwaak")

    if not os.path.exists(agent_path):
      # we use cargo build to ensure the agent is built for the x96_64 architecture
      logging.info(f"Building agent in {agent_root}")
      subprocess.run(["cross","build", "--target", "x86_64-unknown-linux-gnu", "--release"], cwd=agent_root)

    subprocess.run(["cp", agent_path, self.container.instance_dir])
    self.container.exec("chmod +x /tmp/kwaak")
    self.container.exec("cp /tmp/kwaak /usr/local/bin/kwaak")

  def run_agent(self) -> None:
    """Execute the Kwaak agent in the test environment.
    
    This method:
    1. Sets up the agent configuration
    2. Runs the agent in run-agent mode
    3. Provides the problem statement as initial message
    
    """
    template_path = os.path.join(os.path.dirname(__file__), "kwaak.template.toml")
    with open(template_path, "r") as f:
      template = f.read()

    template_path = os.path.join(self.container.instance_dir, "kwaak.rendered.toml")
    with open(template_path, "w") as f:
      f.write(template)

    self.invoke_kwaak()

  def invoke_kwaak(self):
    prompt = self.render_prompt()
    result = self.container.exec(
      f"kwaak -c /tmp/kwaak.rendered.toml run-agent --initial-message \"$PROMPT\" 2>&1 | tee -a /tmp/kwaak.log",
      env={
        "PROMPT": prompt,
        "KWAAK_CACHE_DIR": f"/tmp/kwaak_cache/{self.item.repo}",
        "OPENAI_API_KEY": os.environ["OPENAI_API_KEY"],
      }
    )
    agent_result_path = os.path.join(self.results_dir, "agent_result.txt")
    
    with open(agent_result_path, "w") as f:
      f.write(result.output.decode())
      f.write(f"\nExit Code: {result.exit_code}")

  def render_prompt(self):
    return (
        "A user has reported the following issue:\n\n"
        f"<issue>\n{self.item.problem_statement}\n</issue>\n\n"
        "Could you solve the issue? I have added a failing test case for it. Using the following patch:\n\n"
        f"<patch>\n{self.item.test_patch}\n</patch>\n\n"
        "Please make sure that your solution makes the test(s) in this patch pass, "
        "and does not introduce any new failing tests. "
        "You can ignore tests that were already failing that are not related to the tests in this patch."
        "Do not modify the tests in this patch nor any other tests in the repository, only fix the issue."
    )

  def evaluate_results(self, prediction: dict, results_path: str) -> TrialResult:
    """Evaluate the trial results using SWE-bench grading.
    
    Args:
        prediction: Dictionary containing the agent's patch and metadata
        results: String output from the test execution
    
    Returns:
        TrialResult: Evaluation results including success status and patch
        
    This method:
    1. Prepares the test specification
    2. Gets the evaluation report from SWE-bench
    3. Determines if tests were resolved
    4. Saves the evaluation report
    """
    instance_id = self.item.instance_id
    
    test_spec = make_test_spec(self.item.to_dict())

    logging.info(f"test_spec: {test_spec}")
    report = get_eval_report(test_spec, prediction, results_path, include_tests_status=True)
    logging.info(f"report: {report}")
    resolved = report[instance_id]['resolved']

    logging.info(
      f"report: {report}\n"
      f"Result for {instance_id}: resolved: {resolved}"
    )

    report_path = os.path.join(self.results_dir, f"{self.name}-report.json")
    
    with open(report_path, "w") as f:
      json.dump(report, f, indent=2)

    return TrialResult(
      instance=self.item,
      run_failed=False,
      validation_failed=False,
      patch=prediction["model_patch"],
      success=resolved,
      error=None
    )
