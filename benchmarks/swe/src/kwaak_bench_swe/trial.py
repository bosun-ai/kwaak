"""Trial execution and result management for SWE-bench evaluation.

This module provides classes for executing and evaluating individual test trials:
- TrialResult: Represents the outcome of a trial execution
- Trial: Manages the execution of a single test case

The module handles:
- Docker container lifecycle
- Test environment setup
- Patch application and validation
- Test execution and result collection
- Result evaluation and grading

Typical usage:
    trial = Trial(instance, "test-1", "./results")
    result = trial.run()
    if result.success:
        print("Test passed!")
"""

from dataclasses import dataclass, asdict
from typing import Any
from .swe_bench_instance import SWEBenchInstance
from .docker_instance import DockerInstance
import logging

from swebench.harness.grading import get_eval_report

@dataclass
class TrialResult:
    """Represents the outcome of a trial execution.
    
    This class encapsulates all information about a trial's execution,
    including success/failure status, error messages, and the generated patch.
    
    Attributes:
        instance: The SWE-bench instance that was tested
        run_failed: Whether there was an error during execution
        validation_failed: Whether the test validation failed
        success: Whether the trial successfully passed all tests
        error: Error message if any failure occurred
        patch: The patch generated by the agent
    """
    instance: SWEBenchInstance
    run_failed: bool = False
    validation_failed: bool = False
    success: bool = False
    error: str | None = None
    patch: str | None = None

    def failed(self) -> bool:
      """Check if the trial failed in any way.
      
      Returns:
          bool: True if there was any kind of failure (run, validation, or error),
                False if the trial completed successfully
      """
      return self.run_failed or self.validation_failed or (self.error is not None)

    def to_dict(self) -> dict[str, Any]:
      """Convert the TrialResult to a dictionary for JSON serialization.
      
      This method handles nested objects by checking for and using their
      to_dict methods when available.
      
      Returns:
          dict[str, Any]: A JSON-serializable dictionary containing all trial
                         result data, including nested objects
      """
      result = asdict(self)
      # Convert the SWEBenchInstance to a dict if it has a to_dict method
      if hasattr(self.instance, 'to_dict'):
          result['instance'] = self.instance.to_dict()
      return result

class Trial:
  """Manages the execution of a single SWE-bench test case.
  
  This class handles the complete lifecycle of a test trial, including:
  - Docker container setup and cleanup
  - Test environment preparation
  - Patch application and validation
  - Test execution and result collection
  - Result evaluation and grading
  
  The class integrates with the SWE-bench evaluation framework to ensure
  consistent test execution and result reporting.
  """
  item: SWEBenchInstance
  name: str
  container: DockerInstance
  results_dir: str

  def __init__(self, item: SWEBenchInstance, name: str, results_dir: str) -> None:
    """Initialize a new trial.
    
    Args:
        item: The SWE-bench instance to test
        name: Unique identifier for this trial
        results_dir: Directory where results and artifacts will be stored
    """
    self.item = item
    self.name = name
    self.results_dir = results_dir
    self.container = DockerInstance(self.item, self.results_dir)

  def run(self) -> TrialResult:
    """Execute the trial.
    
    This method performs the complete trial execution:
    1. Sets up the Docker container
    2. Applies the test patch
    3. Establishes initial git state
    4. Runs pre-patch tests
    5. Installs and runs the agent
    6. Collects and evaluates results
    
    Returns:
        TrialResult: The complete results of the trial execution
    """
    logging.info(f"Running trial {self.name}")

    self.container.run()

    self.container.write_string_to_file(self.item.test_patch, "/tmp/test.patch")
    patch_result = self.container.exec("git apply /tmp/test.patch")

    initial_git_ref = self.establish_initial_git_ref()

    if patch_result.exit_code != 0:
      logging.info(f"Test Patch failed: {patch_result}")
      return TrialResult(
        instance=self.item,
        run_failed=False,
        validation_failed=True,
        error="Patch failed",
      )
    
    pre_patch_results = self.container.exec(self.item.test_cmd)
    pre_patch_results_path = os.path.join(self.results_dir, f"{self.name}-pre_patch_test_results.txt")
    
    # write results to file in results_dir
    with open(pre_patch_results_path, "w") as f:
      f.write(pre_patch_results.output)

    # Run the agent
    self.install_agent()
    self.run_agent()

    diff = self.container.exec(f"git diff {initial_git_ref}").output

    prediction = {
      "instance_id": self.item.instance_id,
      "model_name_or_path": self.name,
      "model_patch": diff,
    }

    test_results = self.container.exec(self.item.test_cmd)
    test_results_path = os.path.join(self.results_dir, f"{self.name}-test_results.txt")
    
    with open(test_results_path, "w") as f:
      f.write(test_results.output)

    model_patch_path = os.path.join(self.results_dir, f"{self.name}-patch.diff")

    with open(model_patch_path, "w") as f:
      f.write(diff)

    result = self.evaluate_results(prediction, test_results.output)
    
    # TODO: Uncomment next line when debugging is done:
    # self.container.cleanup()

    return result

  def establish_initial_git_ref(self) -> str:
    """Create an initial git commit and get its reference.
    
    This method:
    1. Configures git user information
    2. Creates a commit with the current state
    3. Returns the commit hash
    
    Returns:
        str: The hash of the created commit
        
    Raises:
        Exception: If git operations fail
    """
    commit_context = "git config user.name 'agent-test-harness'; git config user.email 'agent-test-harness@bosun.ai';"
    result = self.container.exec(f"{commit_context} git commit -a -m \"benchmark-head\" 1>/dev/null; git rev-parse HEAD")
    if result.exit_code != 0:
        raise Exception(f"Failed to establish initial git ref: {result.output}")
    return result.output.strip()

  def install_agent(self) -> None:
    """Install the Kwaak agent in the test environment.
    
    This method:
    1. Locates the agent source in the git repository
    2. Ensures cross-compilation tools are available
    3. Builds the agent for x86_64 Linux
    4. Copies the binary to the container directory
    
    Raises:
        subprocess.CalledProcessError: If any build step fails
    """
    # agent is located at the root of the git repository of the cwd
    agent_root = subprocess.check_output(["git", "rev-parse", "--show-toplevel"]).decode().strip()

    # check if cross is installed
    if subprocess.run(["cross", "--version"], check=False) != 0:
      subprocess.run(["cargo install cross --git https://github.com/cross-rs/cross"], check=True)

    # we use cargo build to ensure the agent is built for the x96_64 architecture
    subprocess.run(["cross", "build", "--target", "x86_64-unknown-linux-gnu", "--release"], cwd=agent_root)
    # copy the agent binary to the root of the results directory
    agent_path = os.path.join(agent_root, "target", "x86_64-unknown-linux-gnu", "release", "kwaak")
    subprocess.run(["cp", agent_path, self.container.instance_dir])

  def run_agent(self) -> None:
    """Execute the Kwaak agent in the test environment.
    
    This method will:
    1. Set up the agent configuration
    2. Run the agent in run-agent mode
    3. Provide the problem statement as initial message
    
    Note: Currently a placeholder for future implementation
    """
    # We setup a kwaak.toml file in the instance directory, then we run the agent
    # using the kwaak command in run-agent mode with an initial-message with a prompt
    # that includes the problem statement from instance.
    pass

  def evaluate_results(self, prediction: dict, results: str) -> TrialResult:
    """Evaluate the trial results using SWE-bench grading.
    
    Args:
        prediction: Dictionary containing the agent's patch and metadata
        results: String output from the test execution
    
    Returns:
        TrialResult: Evaluation results including success status and patch
        
    This method:
    1. Prepares the test specification
    2. Gets the evaluation report from SWE-bench
    3. Determines if tests were resolved
    4. Saves the evaluation report
    """
    instance_id = self.item.instance_id

    test_spec = {
      "instance_id": instance_id,
      "FAIL_TO_PASS": self.item.FAIL_TO_PASS,
      "PASS_TO_PASS": self.item.PASS_TO_PASS,
    }

    report = get_eval_report(test_spec, prediction, results, include_tests_status=True)
    resolved = report[instance_id]['resolved']

    logging.info(
      f"report: {report}\n"
      f"Result for {instance_id}: resolved: {resolved}"
    )

    report_path = os.join(self.results_dir, f"{self.name}-report.json")
    
    with open(report_path, "w") as f:
      json.dump(report, f, indent=2)

    return TrialResult(
      instance=self.item,
      run_failed=False,
      validation_failed=False,
      patch=prediction["model_patch"],
      success=resolved,
      error=None
    )
