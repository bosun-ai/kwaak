{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query pipeline evaluation example notebook\n",
    "\n",
    "This notebook serves as a usueable example to evaluate the Kwaak query pipeline. Evaluations are done with RAGAS and it uses itself as datasource.\n",
    "\n",
    "The intention of this notebook is that it can be modified and tailored for any repository to get a grasp of how the Kwaak rag performs on it.\n",
    "\n",
    "When generating ground truths, it's expected to not be fully correct. The idea is to have a human in the middle tailor it. The benefit is that it gives both a base to work from and an answer that aligns with the output format of the llm.\n",
    "\n",
    "Note: Runs questions only once. If you want volume it needs some tailoring.\n",
    "\n",
    "## How does it work\n",
    "* Generate a RAGAS compatible dataset with recorded ground truths for a set of questions\n",
    "* Review and modify the generated initial answers to establish a ground truth\n",
    "* This stores a /evals/ragas/base.json that will serve as future input\n",
    "* Make some changes to kwaak, run the eval step without recording ground truth, with the base file as input\n",
    "* /evals should then contain ragas/base.json and a json file for each evaluation\n",
    "* Do this as many times as desired, then use the provided analysis (or do it better than me) to make a comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial question generation\n",
    "\n",
    "In this step we will generate our ground truth based on our input questions. A widget is provided to verify and adjust the ground truths.\n",
    "\n",
    "You **must** confirm the ground truths before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Explain kwaak works and explain the architecture. Include a mermaid diagram of all the high level components.\",\n",
    "\"I'd like to be able to configure a session in a file, such that users can add their own custom agents. Create a detailed step-by-step plan.\",\n",
    "\"There are multiple uses of channels in the app. Explore how the channels work, interact, relate and explain it in simpel terms from a users perspective.\",\n",
    "\"How are tools used by an agent?\",\n",
    "\"How can I add a tool for an agent?\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "# Since by default the dir is in benchmarks/notebooks, set our working directory to the root of the project\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install depenencies for the duration of the notebook\n",
    "# Might want to do this proper via uv later!%\n",
    "pip install ragas itertools pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prep these for shell commands so they are double quoted and joined by a comma\n",
    "questions_for_shell = \" \".join([f'-q \"{q}\"' for q in questions])\n",
    "print(questions_for_shell)\n",
    "!RUSTRUST_LOG=debug cargo run --features evaluations --  --allow-dirty eval ragas $questions_for_shell --output=evals/ragas/base_raw.json -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "\n",
    "pd_raw = pd.read_json(\"evals/ragas/base_raw.json\")\n",
    "pd_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a Textarea widget for each ground truth\n",
    "textareas = [widgets.Textarea(value=ground_truth.replace('\\\\n', '\\n'), layout=widgets.Layout(width='100%', height='200px')) for ground_truth in pd_raw[\"ground_truth\"]]\n",
    "pd_base = pd_raw.copy()\n",
    "base_file = \"evals/ragas/base.json\"\n",
    "\n",
    "# Display the Textarea widgets with truncated questions as labels\n",
    "for i, textarea in enumerate(textareas):\n",
    "    question_label = pd_raw[\"question\"][i][:100] + \"...\" if len(pd_raw[\"question\"][i]) > 100 else pd_raw[\"question\"][i]\n",
    "    display(widgets.Label(f\"Question {i+1}: {question_label}\"))\n",
    "    display(textarea)\n",
    "\n",
    "# Function to get the updated ground truths\n",
    "def get_updated_ground_truths():\n",
    "    return [textarea.value for textarea in textareas]\n",
    "\n",
    "# Button to save the updated ground truths\n",
    "save_button = widgets.Button(description=\"Confirm Ground Truths\")\n",
    "display(save_button)\n",
    "\n",
    "def on_save_button_clicked(b):\n",
    "    updated_ground_truths = get_updated_ground_truths()\n",
    "\n",
    "    pd_base = pd_raw.copy()\n",
    "    # Update the dataframe with the new ground truths\n",
    "    columns = pd.Series(updated_ground_truths, name=\"ground_truth\")\n",
    "    pd_base.update(columns)\n",
    "    \n",
    "    # Save the updated dataframe back to the JSON file\n",
    "    pd_base.to_json(base_file, orient='records')\n",
    "    display(widgets.Label(\"Ground truths updated and saved successfully!\"))\n",
    "\n",
    "save_button.on_click(on_save_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature evaluation\n",
    "\n",
    "We use Rust features to evaluate combinations of features to evaluate the result. This is straightforward and allows checking individual and combinations of multiple features quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "features = [\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "# Wtf python, why is this not built in?\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) â†’ () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "# Now we prep an easy to use list of dicts with the { name, features, output_file }\n",
    "evals = []\n",
    "for i, combination in enumerate(list(powerset(features))):\n",
    "    if len(combination) == 0:\n",
    "        continue\n",
    "    evals.append({  \"name\": \"_\".join(combination),\n",
    "                    \"features\": list(combination),\n",
    "                    \"output_file\": f\"evals/ragas/{'_'.join(combination)}.json\"})\n",
    "    \n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for each eval, we will run kwaak with the given features, with the base.json as input, and output to the output file\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "base_file  = \"evals/ragas/base.json\"\n",
    "\n",
    "for eval in evals:\n",
    "    display(widgets.Label(f\"Running evaluation: {eval['name']}, input from {base_file} outputting to {eval['output_file']}\"))\n",
    "    features = \",\".join(eval[\"features\"])\n",
    "    output_file = eval[\"output_file\"]\n",
    "    cmd = f\"RUST_LOG=debug cargo run --features=evaluations,{features} --  --allow-dirty eval ragas -i {base_file} --output={output_file}\"\n",
    "    pprint(cmd)\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load all the evals into a single dataframe, and we add the features as a column\n",
    "pd_all = pd.DataFrame()\n",
    "\n",
    "for eval in evals:\n",
    "    df_eval = pd.read_json(eval[\"output_file\"])\n",
    "    df_eval[\"features\"] = \",\".join(eval[\"features\"])\n",
    "    pd_all = pd.concat([pd_all, df_eval], ignore_index=True)\n",
    "\n",
    "pd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the evaluations on the gathered data for each feature combination\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "# Convert the dataframe to a Hugging Face dataset\n",
    "hf_dataset = Dataset.from_pandas(pd_all)\n",
    "\n",
    "# Now let's add evaluation metrics to each row in the dataset\n",
    "all_results = []\n",
    "for features, df in pd_all.groupby(\"features\"):\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    results = evaluate(hf_dataset, metrics=[answer_relevancy, faithfulness, context_recall, context_precision]).to_pandas()\n",
    "    results[\"features\"] = features\n",
    "    all_results.append(results)\n",
    "\n",
    "# Convert the updated dataframe back to a Hugging Face dataset\n",
    "all_results\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge all results into a single dataframe\n",
    "merged_results = pd.concat(all_results, ignore_index=True)\n",
    "merged_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Convert the merged_results dataframe to long format for easier plotting\n",
    "metrics_long = pd.melt(merged_results, id_vars=[\"user_input\", \"features\"], \n",
    "                       value_vars=[\"answer_relevancy\", \"faithfulness\", \"context_recall\", \"context_precision\"],\n",
    "                       var_name=\"metric\", value_name=\"value\")\n",
    "\n",
    "# Create a bar plot for the evaluation metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"metric\", y=\"value\", hue=\"features\", data=metrics_long)\n",
    "plt.title(\"Evaluation Metrics by Feature\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.legend(title=\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap so we can see how the data moves\n",
    "heatmap_data = metrics_long.pivot_table(index='metric', columns='features', values='value')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Heatmap of Evaluation Metrics by Feature\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
